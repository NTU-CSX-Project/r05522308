{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding look up loading\n",
    "word2vec_lookup = np.load('word2vec_gensim.npy')\n",
    "word2vec_lookup = word2vec_lookup[None][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "words2oneHot = dict()\n",
    "words = np.array(list(word2vec_lookup.keys()))\n",
    "voca_size = len(words)\n",
    "for i in range(voca_size):\n",
    "    words2oneHot[words[i]] = np.zeros(voca_size).astype('int')\n",
    "    words2oneHot[words[i]][i] = 1\n",
    "oneHotPos2vec_lookup = dict()\n",
    "oneHotPos2words = dict()\n",
    "for key, val in words2oneHot.items():\n",
    "    oneHotPos2vec_lookup[np.argmax(words2oneHot[key])] = word2vec_lookup[key]\n",
    "    oneHotPos2words[np.argmax(words2oneHot[key])] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_list = [list(oneHotPos2vec_lookup[i]) for i in range(len(oneHotPos2vec_lookup.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /mnt/hgfs/進階軟體開發專題/NLP/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u1ad5655d51ba17ccc0ad09208c097d82.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.458 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word segmentation done. len = 6910 1104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6910, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# books vectors\n",
    "sent_len = 10\n",
    "\n",
    "with open('101Matthew.htm', 'r') as f1:\n",
    "    data = list(f1)\n",
    "f1.close()\n",
    "with open('102Mark.htm', 'r') as f2:\n",
    "    data += list(f2)\n",
    "f2.close()\n",
    "with open('103Luke.htm', 'r') as f3:\n",
    "    data += list(f3)\n",
    "f3.close()\n",
    "with open('104John.htm', 'r') as f4:\n",
    "    data += list(f4)\n",
    "f4.close()\n",
    "sentences = list()\n",
    "for i in range(len(data)):\n",
    "    data[i] = re.sub(r'[<>『』(）《*/\"=!@#%:.？…、“，\\n”‘’: |[-]|\\u3000]|[a-zA-Z0-9]|[\\u3000]', r'', data[i])\n",
    "    data[i]=re.split('。', data[i])\n",
    "    for element in data[i]:\n",
    "        if element == '':\n",
    "            data[i].remove(element)\n",
    "    sentences += data[i]\n",
    "print('Pre-processing done.')\n",
    "sent_seg = list()\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "sent_seg_too_long = list()\n",
    "f_s = list()\n",
    "for i in range(len(sentences)):\n",
    "    s = sentences[i]\n",
    "    s = list(jieba.cut(s))\n",
    "    if len(s) > sent_len:\n",
    "        f = int(len(s)/sent_len)\n",
    "        f_ = sent_len - len(s) + int(len(s)/sent_len)*sent_len\n",
    "        for j in range(f):\n",
    "            sent_seg.append(s[j*sent_len:j*sent_len+sent_len])\n",
    "            f_s.append(0)\n",
    "        unknowns = ['<unknown>' for k in range(f_)]\n",
    "        if len(unknowns) >= sent_len/2:\n",
    "            continue\n",
    "        sent_seg.append(s[j*sent_len+sent_len:]+unknowns)\n",
    "        sent_seg_too_long.append(s)\n",
    "    else:\n",
    "        f_ = 0\n",
    "        unknowns = ['<unknown>' for j in range(sent_len-len(s))]\n",
    "        s += unknowns\n",
    "        sent_seg.append(s)\n",
    "    f_s.append(f_)\n",
    "sent_seg = np.array(sent_seg)\n",
    "print('word segmentation done.', 'len =', len(sent_seg), len(sent_seg_too_long))\n",
    "sent_seg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5806, 1: 247, 2: 251, 3: 278, 4: 328}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__ = dict()\n",
    "for i in f_s:\n",
    "    if i in __:\n",
    "        __[i] += 1\n",
    "    else:\n",
    "        __[i] = 1\n",
    "__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_X(i, batch):\n",
    "    batch_x = list()\n",
    "    for s in sent_seg[i:i+batch]:\n",
    "        v = list()\n",
    "        for w in s:\n",
    "            if w is '<unknown>':\n",
    "                v.append(vec_list[np.random.randint(len(vec_list))])\n",
    "            elif w in word2vec_lookup:\n",
    "                v.append(word2vec_lookup[w])\n",
    "            else:\n",
    "                v.append(vec_list[np.random.randint(len(vec_list))])\n",
    "        batch_x.append(v)\n",
    "    batch_x = np.array(batch_x)\n",
    "    return batch_x\n",
    "def batch_Y(i, batch):\n",
    "    batch_y = list()\n",
    "    for s in sent_seg[i+1:i+batch+1]:\n",
    "        v = list()\n",
    "        for w in s:\n",
    "            if w is '<unknown>':\n",
    "                v.append(np.random.randint(len(vec_list)))\n",
    "            elif w in words2oneHot:\n",
    "                v.append(np.argmax(words2oneHot[w]))\n",
    "            else:\n",
    "                v.append(np.random.randint(len(vec_list)))\n",
    "        batch_y.append(v)\n",
    "    batch_y = np.array(batch_y)\n",
    "    return batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 10, 1024),\n",
       " array([[ 0.00139729, -0.00304426,  0.00319758, ...,  0.00546759,\n",
       "          0.0030041 , -0.00779329],\n",
       "        [ 0.00154612, -0.00377121,  0.00481181, ...,  0.00671196,\n",
       "          0.00388792, -0.00995091],\n",
       "        [ 0.07981644, -0.14293131,  0.19046581, ...,  0.29229102,\n",
       "          0.17591159, -0.42922705],\n",
       "        ..., \n",
       "        [ 0.0024073 , -0.00417157,  0.00470921, ...,  0.00731867,\n",
       "          0.00446155, -0.01117776],\n",
       "        [ 0.07981644, -0.14293131,  0.19046581, ...,  0.29229102,\n",
       "          0.17591159, -0.42922705],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]], dtype=float32),\n",
       " array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ..., \n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]], dtype=bool))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x = batch_X(0, 50)\n",
    "batch_x2 = batch_X(1, 50)\n",
    "batch_y = batch_Y(0, 50)\n",
    "batch_x.shape, batch_x[45], batch_x[1]==batch_x2[0] # (batch size, sent len, vec len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = len(word2vec_lookup['<unknown>'])\n",
    "oneHot_size = len(words2oneHot['<unknown>'])\n",
    "enc_len = sent_len\n",
    "dec_len = enc_len\n",
    "n_layer1 = 1024\n",
    "l_r = 1e-3\n",
    "epoch = 500\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random weight and bias\n",
    "def _weight(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=1.)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def _bias(shape):\n",
    "    initial = tf.constant(1., shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vectors constant\n",
    "vecs = tf.constant(vec_list)\n",
    "\n",
    "# input - word vectors\n",
    "word_vec = [tf.placeholder(tf.float32, shape=[None, vec_size], name='word_vec') for i in range(enc_len)]    \n",
    "\n",
    "# encoder input\n",
    "w_enc = _weight([vec_size, n_layer1])\n",
    "b_enc = _bias([n_layer1])\n",
    "enc_inp = [tf.nn.dropout(tf.sigmoid(tf.matmul(word_vec[i], w_enc)+b_enc), keep_prob=.8, name='enc_inp') \\\n",
    "           for i in range(enc_len)]\n",
    "\n",
    "# training target - word one-hot\n",
    "word_target = [tf.placeholder(tf.int32, shape=[None], name='word_target') for i in range(dec_len)]\n",
    "\n",
    "# one-hot to word vectors\n",
    "target = [tf.nn.embedding_lookup(vecs, word_target[i]) for i in range(dec_len-1)]\n",
    "target = [tf.zeros_like(target[0], dtype=np.float32, name=\"GO\")] + target\n",
    "\n",
    "# decoder input\n",
    "w_dec = _weight([vec_size, n_layer1])\n",
    "b_dec = _bias([n_layer1])\n",
    "dec_inp = [tf.nn.dropout(tf.nn.relu(tf.matmul(target[i], w_dec)+b_dec), keep_prob=.8, name='dec_inp') \\\n",
    "           for i in range(dec_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hiddens = 3\n",
    "cells = [tf.contrib.rnn.BasicLSTMCell(n_layer1, forget_bias=1.0) for i in range(n_hiddens)]\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "dec_outputs, dec_memory = tf.contrib.legacy_seq2seq.basic_rnn_seq2seq(\n",
    "        enc_inp, \n",
    "        dec_inp, \n",
    "        cell\n",
    "    )\n",
    "\n",
    "w_dec_o = _weight([n_layer1, oneHot_size])\n",
    "b_dec_o = _weight([oneHot_size])\n",
    "dec_outputs = [tf.matmul(dec_outputs[i], w_dec_o)+b_dec_o for i in range(dec_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_20:0\", shape=(?, 6306), dtype=float32) Tensor(\"word_target:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"add_21:0\", shape=(?, 6306), dtype=float32) Tensor(\"word_target_1:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"add_22:0\", shape=(?, 6306), dtype=float32) Tensor(\"word_target_2:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"add_23:0\", shape=(?, 6306), dtype=float32) Tensor(\"word_target_3:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"add_24:0\", shape=(?, 6306), dtype=float32) Tensor(\"word_target_4:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"add_25:0\", shape=(?, 6306), dtype=float32) Tensor(\"word_target_5:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"add_26:0\", shape=(?, 6306), dtype=float32) Tensor(\"word_target_6:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"add_27:0\", shape=(?, 6306), dtype=float32) Tensor(\"word_target_7:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"add_28:0\", shape=(?, 6306), dtype=float32) Tensor(\"word_target_8:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"add_29:0\", shape=(?, 6306), dtype=float32) Tensor(\"word_target_9:0\", shape=(?,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "loss = list()\n",
    "loss_all = 0\n",
    "accu = list()\n",
    "for y_hat, y_real in zip(dec_outputs, word_target):\n",
    "    #loss.append(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat, labels=y_real)))\n",
    "    print(y_hat, y_real)\n",
    "    #accu += tf.reduce_mean(tf.cast(tf.equal(y_hat, y_real), tf.float32))\n",
    "    loss_all += tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_hat, labels=y_real)\n",
    "loss_all = tf.reduce_mean(loss_all/dec_len)\n",
    "lr_decay = 0.92  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.5  # default: 0.0 . Momentum technique in weights update\n",
    "op = tf.train.RMSPropOptimizer(l_r, decay=lr_decay, momentum=momentum).minimize(loss_all)\n",
    "#ops = [tf.train.AdamOptimizer(l_r).minimize(l) for l in loss]\n",
    "#op3 = tf.train.AdagradOptimizer(l_r).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'RMSProp' type=NoOp>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op#, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch: 1 , iter: 50 , loss: [7.243679]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 肥肥肥．．．．．．．\n",
      "--------------------------Time span: 1119.944514989853 --------------------------\n",
      "Epoch 2\n",
      "Epoch: 2 , iter: 50 , loss: [38.62384]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 有有和和和和和和和和\n",
      "--------------------------Time span: 1147.1416835784912 --------------------------\n",
      "Epoch 3\n",
      "Epoch: 3 , iter: 50 , loss: [6.9828448]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 耶穌說說說<unknown><unknown><unknown><unknown><unknown><unknown>\n",
      "--------------------------Time span: 1097.3119313716888 --------------------------\n",
      "Epoch 4\n",
      "Epoch: 4 , iter: 50 , loss: [6.8125558]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 耶穌說說說說說說的說說\n",
      "--------------------------Time span: 1096.0096981525421 --------------------------\n",
      "Epoch 5\n",
      "Epoch: 5 , iter: 50 , loss: [6.0003629]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的的的的的的的的的的\n",
      "--------------------------Time span: 1097.9076302051544 --------------------------\n",
      "Epoch 6\n",
      "Epoch: 6 , iter: 50 , loss: [5.835309]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 他們說說你你你你你你你\n",
      "--------------------------Time span: 1092.8576352596283 --------------------------\n",
      "Epoch 7\n",
      "Epoch: 7 , iter: 50 , loss: [5.7556915]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的說你你你你你的你你\n",
      "--------------------------Time span: 1099.5847294330597 --------------------------\n",
      "Epoch 8\n",
      "Epoch: 8 , iter: 50 , loss: [5.726347]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 他們說你你你你你的你你\n",
      "--------------------------Time span: 1112.6488749980927 --------------------------\n",
      "Epoch 9\n",
      "Epoch: 9 , iter: 50 , loss: [5.7011094]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的說你你你你你的你你\n",
      "--------------------------Time span: 1083.6390450000763 --------------------------\n",
      "Epoch 10\n",
      "Epoch: 10 , iter: 50 , loss: [5.668611]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: ．說你你你你你你<unknown><unknown>\n",
      "--------------------------Time span: 1082.5308108329773 --------------------------\n",
      "Epoch 11\n",
      "Epoch: 11 , iter: 50 , loss: [5.6448603]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: ．說說你的你你的<unknown><unknown>\n",
      "--------------------------Time span: 1086.0354702472687 --------------------------\n",
      "Epoch 12\n",
      "Epoch: 12 , iter: 50 , loss: [5.6347299]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的說說說說說<unknown>的<unknown><unknown>\n",
      "--------------------------Time span: 1095.2157516479492 --------------------------\n",
      "Epoch 13\n",
      "Epoch: 13 , iter: 50 , loss: [5.6218157]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的說說說的的<unknown>的<unknown><unknown>\n",
      "--------------------------Time span: 1088.4795472621918 --------------------------\n",
      "Epoch 14\n",
      "Epoch: 14 , iter: 50 , loss: [5.6044607]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的說說說的的<unknown>的<unknown><unknown>\n",
      "--------------------------Time span: 1087.557734966278 --------------------------\n",
      "Epoch 15\n",
      "Epoch: 15 , iter: 50 , loss: [5.5728207]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 耶穌說說說的的<unknown>的<unknown><unknown>\n",
      "--------------------------Time span: 1091.1583003997803 --------------------------\n",
      "Epoch 16\n",
      "Epoch: 16 , iter: 50 , loss: [5.5686035]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的說說的的的<unknown>的的<unknown>\n",
      "--------------------------Time span: 1087.623209476471 --------------------------\n",
      "Epoch 17\n",
      "Epoch: 17 , iter: 50 , loss: [5.5461788]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 耶穌說說的的的的的<unknown><unknown>\n",
      "--------------------------Time span: 1104.9125864505768 --------------------------\n",
      "Epoch 18\n",
      "Epoch: 18 , iter: 50 , loss: [5.5001812]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 耶穌說說的的的<unknown><unknown><unknown><unknown>\n",
      "--------------------------Time span: 1165.9121725559235 --------------------------\n",
      "Epoch 19\n",
      "Epoch: 19 , iter: 50 , loss: [5.4881372]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的若說的的的的的的<unknown>\n",
      "--------------------------Time span: 1137.5610604286194 --------------------------\n",
      "Epoch 20\n",
      "Epoch: 20 , iter: 50 , loss: [5.4506168]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的若他們的的的的的的<unknown>\n",
      "--------------------------Time span: 1125.8679308891296 --------------------------\n",
      "Epoch 21\n",
      "Epoch: 21 , iter: 50 , loss: [5.4604635]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的若了的的的的的的<unknown>\n",
      "--------------------------Time span: 1146.9619035720825 --------------------------\n",
      "Epoch 22\n",
      "Epoch: 22 , iter: 50 , loss: [5.4284348]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的若了的的的的的的的\n",
      "--------------------------Time span: 1159.6580801010132 --------------------------\n",
      "Epoch 23\n",
      "Epoch: 23 , iter: 50 , loss: [5.4100571]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的要了的的的的的的的\n",
      "--------------------------Time span: 1127.8659241199493 --------------------------\n",
      "Epoch 24\n",
      "Epoch: 24 , iter: 50 , loss: [5.3754482]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的說了的的的的<unknown>的<unknown>\n",
      "--------------------------Time span: 1108.7423179149628 --------------------------\n",
      "Epoch 25\n",
      "Epoch: 25 , iter: 50 , loss: [5.4236875]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 他說了的的的的<unknown><unknown>的\n",
      "--------------------------Time span: 1104.7524466514587 --------------------------\n",
      "Epoch 26\n",
      "Epoch: 26 , iter: 50 , loss: [5.2778349]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 的要了的的的的的的的\n",
      "--------------------------Time span: 1103.9248192310333 --------------------------\n",
      "Epoch 27\n",
      "Epoch: 27 , iter: 50 , loss: [5.2606683]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 他說了說的的的<unknown><unknown><unknown>\n",
      "--------------------------Time span: 1100.5344944000244 --------------------------\n",
      "Epoch 28\n",
      "Epoch: 28 , iter: 50 , loss: [5.2489877]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 他說了的的的的<unknown><unknown>的\n",
      "--------------------------Time span: 1104.561063528061 --------------------------\n",
      "Epoch 29\n",
      "Epoch: 29 , iter: 50 , loss: [5.1700993]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 他的了的的的的的的的\n",
      "--------------------------Time span: 1119.3300449848175 --------------------------\n",
      "Epoch 30\n",
      "Epoch: 30 , iter: 50 , loss: [5.1595449]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 他要了的的的的<unknown>的的\n",
      "--------------------------Time span: 1114.6393184661865 --------------------------\n",
      "Epoch 31\n",
      "Epoch: 31 , iter: 50 , loss: [5.0920634]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 他說了說的的<unknown><unknown><unknown><unknown>\n",
      "--------------------------Time span: 1129.7670903205872 --------------------------\n",
      "Epoch 32\n",
      "Epoch: 32 , iter: 50 , loss: [5.0745182]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 我說我我的說<unknown><unknown><unknown><unknown>\n",
      "--------------------------Time span: 1110.2753582000732 --------------------------\n",
      "Epoch 33\n",
      "Epoch: 33 , iter: 50 , loss: [4.9882908]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 你說了說的說<unknown><unknown><unknown><unknown>\n",
      "--------------------------Time span: 1111.878178358078 --------------------------\n",
      "Epoch 34\n",
      "Epoch: 34 , iter: 50 , loss: [4.9420872]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 他說了說的說<unknown><unknown><unknown><unknown>\n",
      "--------------------------Time span: 1245.227739572525 --------------------------\n",
      "Epoch 35\n",
      "Epoch: 35 , iter: 50 , loss: [4.8647475]\n",
      "Input: 恐怕他的門徒來把他偷了去\n",
      "Output: 我也我我的的<unknown><unknown><unknown><unknown>\n",
      "--------------------------Time span: 1135.177960395813 --------------------------\n",
      "Epoch 36\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epo in range(epoch):\n",
    "    print('Epoch', epo+1)\n",
    "    t_start = time.time()\n",
    "    for i in range(int(len(sent_seg)/batch_size)):\n",
    "        batch_x = batch_X(i*batch_size, batch_size)\n",
    "        batch_y = batch_Y(i*batch_size, batch_size)\n",
    "        feed_dict = {word_vec[t]: batch_x[:, t] for t in range(enc_len)}\n",
    "        feed_dict.update({word_target[t]: batch_y[:, t] for t in range(dec_len)})\n",
    "        \n",
    "        _ = sess.run([op], feed_dict=feed_dict)\n",
    "        #lo, ac = sess.run([loss, accu], feed_dict=feed_dict)\n",
    "        if (i+1)%50 == 0:\n",
    "            lo = sess.run([loss_all], feed_dict=feed_dict)\n",
    "            print('Epoch:', epo+1, \n",
    "                  ', iter:', i+1, \n",
    "                  ', loss:', lo)\n",
    "        #if (epo+1)%10 == 0:\n",
    "    #-----test-----\n",
    "    batch_x = batch_X(1886, 1)\n",
    "    batch_y = batch_Y(1886, 1)\n",
    "    feed_dict = {word_vec[t]: batch_x[:, t] for t in range(enc_len)}\n",
    "    feed_dict.update({word_target[t]: batch_y[:, t] for t in range(dec_len)})\n",
    "    test = sess.run([dec_outputs], feed_dict=feed_dict)\n",
    "    test = test[0]\n",
    "    w_ = list()\n",
    "    for i in range(sent_len):\n",
    "        w_.append(test[i][0])\n",
    "    print('Input: '+''.join(sent_seg[1886]))\n",
    "    print('Output: '+''.join([oneHotPos2words[np.argmax(w)] for w in w_ ]))\n",
    "    t_end = time.time()\n",
    "    print('--------------------------Time span:', t_end-t_start, '--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './models/'+time.strftime('%Y.%m.%d_%H%M%S')+'novel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
